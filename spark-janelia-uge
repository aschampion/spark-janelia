#!/usr/bin/env python2.7

import os
import glob
import sys
import argparse
import subprocess
import xml.etree.ElementTree as ET
import time
import multiprocessing
import traceback


SPARK_ROOT = '/lmb/home/achampion/spark'
START_MASTER_SCRIPT = SPARK_ROOT + '/sge/start-flexmaster.sh'
START_WORKER_SCRIPT = SPARK_ROOT + '/sge/start-flexworker.sh'

SPARK_NAME_PREFIX = 'spark'
MASTER_NAME = SPARK_NAME_PREFIX + '.master'
WORKER_NAME_PREFIX = SPARK_NAME_PREFIX + '.worker'
DRIVER_NAME_PREFIX = SPARK_NAME_PREFIX + '.driver'

def worker_name_for(masterId):
    return WORKER_NAME_PREFIX + '.' + str(masterId) + '.'

def driver_name_for(masterId):
    return DRIVER_NAME_PREFIX + '.' + str(masterId) + '.'

class Job:
    def __init__(self, name, state, id, stat):
        self.name = name
        self.state = state
        self.id = id
        self.stat = stat

    @classmethod
    def from_stat(cls, job):
        name = job.find('JB_name').text
        state = job.find('state').text
        jobid = int(job.find('JB_job_number').text)

        return cls(name, state, jobid, job)
    
    def __getitem__(self, key):
        return self.stat.find(key).text

    def is_running(self):
        return self.state == 'r'

    def is_pending(self):
        return 'h' in self.state or 'w' in self.state

    def is_spark(self):
        return self.name.startswith(SPARK_NAME_PREFIX)

    def is_spark_master(self):
        return self.name.startswith(MASTER_NAME)

    def is_spark_worker(self):
        return self.name.startswith(SPARK_WORKER_PREFIX)

    def is_spark_worker_for(self, masterId):
        workerName = worker_name_for(masterId)
        return self.name.startswith(workerName)

    def is_spark_driver_for(self, masterId):
        driverName = driver_name_for(masterId)
        return self.name.startswith(driverName)

    def host(self):
        return self['queue_name'].split('@')[1]

def spark_jobs():
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found, keep waiting for jobs to launch"
        print('\n')
    else:
        for job in jobs:
            job = Job.from_stat(job)
            if job.is_spark():
                yield job

def running_spark_jobs():
    for job in spark_jobs():
        if job.is_running():
            yield job

def spark_masters():
    for job in spark_jobs():
        if job.is_spark_master():
            yield job

def spark_workers_for(masterJobID):
    for job in spark_jobs():
        if job.is_spark_worker_for(masterJobID):
            yield job

def spark_drivers_for(masterJobID):
    for job in spark_jobs():
        if job.is_spark_driver_for(masterJobID):
            yield job

def spark_url_for_host(host):
    return 'spark://{}:7077'.format(host.split('.')[0])

def find_master_host_by_id(jobID):
    for job in running_spark_jobs():
        if jobID == '' or job.id == jobID:
            return job.host()
    return None


def launchorig(sleepTime='86400'):
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    startupCommand = '/misc/local/spark-versions/bin/' + launchscript
    jobID = subprocess.check_output(['qsub', '-terse', '-o', os.path.expanduser('~/sparklogs/'), startupCommand])
    print('\n')
    print('Spark job submitted with ' + str(args.nnodes) + ' nodes')
    print('\n')
    return jobID

def launch(runtime):
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    sparktype = args.version
    masterJobID = start_master(sparktype, runtime)
    time.sleep(5)
    try:
        for i in range(args.nnodes):
            start_worker(sparktype, masterJobID, runtime)
    except:
        print "No workers specified or other error"
        traceback.print_exc()
        sys.exit(1)
    return masterJobID

def common_qsub_options(slots):
    return ['qsub', '-o', os.path.expanduser('~/sparklogs'), '-terse', 
        '-v', 'SPARK_HOME='+version, '-b', 'y', 
        '-l', 'dedicated='+str(slots)]

def start_master(sparktype, runtime):
    command = common_qsub_options(args.masterslots) + ['-N', MASTER_NAME, START_MASTER_SCRIPT]
    print ' '.join(command)
    masterJobID = subprocess.check_output(command)
    print "Master submitted. Job ID is {}".format(masterJobID)
    return int(masterJobID)

def start_worker(sparktype, masterJobID, runtime):
    if not any(job.id == masterJobID for job in spark_masters()):
        print "No master with the job id {} running or queued. Please submit master first.".format(masterJobID)
        sys.exit()

    workerName = worker_name_for(masterJobID)

    # TODO: runtime
    command = common_qsub_options(args.workerslots) + ['-N', workerName, START_WORKER_SCRIPT]
    print ' '.join(command)
    subprocess.check_output(command)


def login():
    address = None
    for job in running_spark_jobs():
        if args and job.id == args.jobid:
            address = job.host() 
    if address:
        filename = os.path.expanduser('~/spark-master')
        if os.path.exists(filename):
            os.remove(filename)
        os.system("echo '{}' >> {}".format(spark_url_for_host(address), filename))
        subprocess.call(['qlogin', '-pe', 'batch', '16', '-l', 'interactive=true'])
    else:
        print('\n')
        print >> sys.stderr, "No Spark job found, check status with qstat, or try a different jobid?"
        print('\n')


def start(command = 'pyspark'):
    masters = list(spark_masters())
    if len(masters) == 0:
        print >> sys.stderr, "No master found. If already reqest, please wait for master to be acquired. Otherwise, use spark-janelia launch."
        sys.exit()
    else:
        master = master[0]

    if args.notebook is True or args.ipython is True:
       os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython'

    if args.notebook is True:
       os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'
       os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook --ip "*" --port 9999 --no-browser'
       print('\n')
       print('View your notebooks at http://' + os.environ['HOSTNAME'] + ':9999')
       print('View the status of your cluster at http://' + master.host() + ':8080')
       print('\n')

    os.system(version + '/bin/' + command + ' --master=' + spark_url_for_host(master.host()))

def set_environment(masterJobID):
    #Set MASTER
    if "MASTER" not in os.environ:
        os.environ["MASTER"] = spark_url_for_host(find_master_host_by_id(masterJobID))
    
    #Set SPARK_HOME
    if "SPARK_HOME" not in os.environ:
        os.environ["SPARK_HOME"] = version

    #Set PATH
    sparkpath = "{}/bin:{}/sbin".format(os.getenv('SPARK_HOME'), os.getenv('SPARK_HOME')) 
    if sparkpath not in os.environ['PATH']:
        os.environ["PATH"] = str("{}:{}".format(sparkpath, os.environ['PATH']))

def submit(jobID, sparkcommand, runtime):
    set_environment(jobID)
    driverName = driver_name_for(jobID)
    command = common_qsub_options(args.driverslots) + ['-N', driverName, '-V', '-j', 'y']
    if args.driveroutfile is not '':
        command += ['-o', args.driveroutfile]
    try:
        command += [sparkcommand]
        print(' '.join(command))
        driverJobID = subprocess.check_output(command, universal_newlines=True)
        print('Driver submitted. Job ID is {}'.format(driverJobID))
    except:
        print("Driver failed to launch. Cluster is still running.")
        traceback.print_exc()
        sys.exit(1)
    return driverJobID


def launch_and_wait():
    jobID = launch(str(args.hard_runtime))
    master = ''
    while( master == '' ):
        master = find_master_host_by_id(jobID)
        time.sleep(10) # wait to avoid spamming the cluster
        sys.stdout.write('.')
        sys.stdout.flush()

    enoughworkers = False
    while enoughworkers == False:
        time.sleep(10)
        workerlist = spark_workers_for(jobID)
        rworkercount = len([worker for worker in workerlist if worker.is_running()])
        print('There are {} workers running.'.format(rworkercount))
        if rworkercount >= args.minworkers:
            print('Sufficient workers')
            enoughworkers = True

    return master, jobID

def check_for_update():
    currentdir = os.getcwd()
    scriptdir = os.path.dirname(os.path.realpath(__file__))
    os.chdir(scriptdir)
    output = subprocess.check_output('git fetch --dry-run 2>&1', universal_newlines=True, shell=True) 
    os.chdir(currentdir)
    if "origin" in output: 
         print("This script is not up to date. To update the script, run spark-janelia update.")

def update():
    currentdir = os.getcwd()
    scriptdir = os.path.dirname(os.path.realpath(__file__))
    os.chdir(scriptdir)
    os.system('git pull')
    os.chdir(currentdir)

def stop_worker(masterJobID, terminatew, skipcheckstop):
    jobtokill = ""
    workers = list(spark_workers_for(masterJobID))

    if any(w.is_pending() for w in workers) and not skipcheckstop:
        terminatew = raw_input("Terminate waiting job(s) first? (y/n) ")

    for job in workers:
        if job.is_pending() and terminatew == 'y':
            jobtokill = job.id
            break
        elif job.is_running() and not skipcheckstop:
            jobtokill = selection_list(workerlist, 'worker')
            break
        else:
            jobtokill = job.id
            break
    try:
        if jobtokill != "":
            qdel_job(jobtokill)
            time.sleep(3)
    except:

        print "No running or waiting workers found corresponding to Master job {}".format(masterJobID)
        traceback.print_exc()
        sys.exit()
    return terminatew

def qdel_job(jobID):
    subprocess.check_output(['qdel', str(jobID)])


def checkstop(inval, jobtype):
    if jobtype == "master":
        checkstop = raw_input("Stop master with job id {} (y/n):".format(inval))
    else:
        checkstop = raw_input("Remove {} workers? (y/n):".format(inval))
    if checkstop != "y":
        print "No workers will be removed."
        sys.exit()
    else:
        return True

def selection_list(joblist, jobtype):
    selectionlist = {}
    print "Select {} to kill from list below:".format(jobtype)
    for (i, job) in enumerate(joblist):
        selectionlist[i] = job.id
        print "{}) Host: {} JobID: {} State: {}".format(i, job.host(), job.id, job.state)
    while True:
        selection = int(raw_input("Selection? "))
        if selection <= len(selectionlist):
            jobID = selectionlist[selection]
            skipcheckstop = True
            break
        else:
            print "Invalid selection."
    return jobID

def destroy(masterJobID):
    jobtype = "master"
    try:
        qdel_job(masterJobID)
    except:
        print "Master with job id {} not found".format(masterJobID)
        sys.exit(1)
    try:
        workername = worker_name_for(masterJobID)
        qdel_job(workername)
    except:
        print "Workers for master with job id {} failed to stop".format(masterJobID)
        sys.exit(1)
    try:
        drivername = driver_name_for(masterJobID)
        qdel_job(drivername)
    except:
        print "Drivers for master with job id {} failed to stop".format(masterJobID)
        sys.exit(1)


def submit_and_destroy(jobID, sparksubargs, runtime):
    driverjobID = submit(jobID, sparksubargs, runtime)
    drivercomplete = False
    while not drivercomplete:
        try:
            subprocess.check_output(["qstat",  "-j", str(driverjobID)])
        except subprocess.CalledProcessError:
            # qstat returns 1 for finished jobs.
            drivercomplete = True
        time.sleep(30)
    destroy(jobID)

if __name__ == "__main__":

    skipcheckstop = False

    parser = argparse.ArgumentParser(description="launch and manage spark cluster jobs")

    choices = ('launch', 'launchall', 'login', 'destroy', 'start', 'start-scala', 'submit', 'lsd', 'launch-in', 'update', 'add-workers', 'remove-workers', 'stopcluster')

    parser.add_argument("task", choices=choices)
    parser.add_argument("-n", "--nnodes", type=int, default=2, required=False)
    parser.add_argument("-i", "--ipython", action="store_true")
    parser.add_argument("-b", "--notebook", action="store_true")
    parser.add_argument("-v", "--version", choices=("current"), default="current", required=False)
    parser.add_argument("-j", "--jobID", type=int, default=None, required=False)
    parser.add_argument("-t", "--hard_runtime", type=str, default=86400, required=False)
    parser.add_argument("-s", "--submitargs", type=str, default='', required=False)
    parser.add_argument("-f", "--force", action="store_true")
    parser.add_argument("-o", "--driveroutfile", type=str, default='', required=False)
    parser.add_argument("--no_check", action="store_true")
    parser.add_argument("--masterslots", type=int, default=4, required=False)
    parser.add_argument("--workerslots", type=int, default=24, required=False)
    parser.add_argument("--driverslots", type=int, default=4, required=False)
    parser.add_argument("--driveronspark", action="store_true")  # TODO: noop for compatibility
    parser.add_argument("--silentlaunch", action="store_true")  # TODO: noop for compatibility
    parser.add_argument("--minworkers", type=int, default=1, required=False)

    args = parser.parse_args()

    SPARKVERSIONS = {
        'current': SPARK_ROOT + '/versions/current',
    }

    version = SPARKVERSIONS[args.version]

    if args.task == 'update':
        update()

    if args.no_check == False:
        check_for_update()

    if args.force == True:
        skipcheckstop = True

    if args.task == 'launch':
        masterjobID = launch(str(args.hard_runtime))
        masterurl = spark_url_for_host(find_master_host_by_id(masterjobID))
        print("To set $MASTER environment variable, enter export MASTER={} at the command line.".format(masterurl))

    elif args.task == 'launchall':
        launchorig(str(args.hard_runtime))

    elif args.task == 'login':
        login()

    elif args.task == 'destroy':
        destroy(args.jobID or '')

    elif args.task == 'start':
        start()

    elif args.task == 'start-scala':
        start('spark-shell')

    elif args.task == 'submit':
        sparksubargs = 'spark-submit {}'.format(args.submitargs)
        submit(args.jobID, sparksubargs, args.hard_runtime)

    elif args.task == 'lsd':
        master, jobID = launch_and_wait()
        master = spark_url_for_host(master)
        print('\n')
        print('%-20s%s\n%-20s%s' % ( 'job id:', jobID, 'spark master:', master ) )
        print('\n')
        sparksubargs = 'spark-submit {}'.format(args.submitargs)
        p = multiprocessing.Process(target=submit_and_destroy, args=(jobID, sparksubargs, args.hard_runtime))
        p.start()

    elif args.task == 'launch-in':
        master, jobID = launch_and_wait()
        print('\n\nspark master UI at: http://{}:8080\n'.format(master))
        login()

    elif args.task == 'add-workers':
        for node in range(args.nnodes):
            start_worker(args.version, args.jobID, args.hard_runtime)

    elif args.task == 'remove-workers':
        terminatew = ""
        jobtype = "worker"
        for node in range(args.nnodes):
            terminatew = stop_worker(args.jobID, terminatew, skipcheckstop)

    elif args.task == 'stopcluster':
        if args.jobID is not None:
            masterJobID = args.jobID
        else:
            masterJobID = selection_list(spark_masters(), 'master')
        if not skipcheckstop:
            checkstop(masterJobID, 'master')
        destroy(masterJobID)
